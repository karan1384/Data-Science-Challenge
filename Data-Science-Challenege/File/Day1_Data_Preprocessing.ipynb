{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X4uzDqV3a0Ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Importing the libraries"
      ],
      "metadata": {
        "id": "Fx-hlaaOa2kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "7l2Yh4_Ia3__"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Importing dataset"
      ],
      "metadata": {
        "id": "DVnsTugTa4gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('Data.csv')\n",
        "X = dataset.iloc[ : , :-1].values\n",
        "Y = dataset.iloc[ : , 3].values"
      ],
      "metadata": {
        "id": "P_QbSyx_a4v2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Handling the missing data"
      ],
      "metadata": {
        "id": "U_XxNtLJa4-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values = np.nan, strategy = \"mean\")\n",
        "imputer = imputer.fit(X[ : , 1:3])\n",
        "X[ : , 1:3] = imputer.transform(X[ : , 1:3])"
      ],
      "metadata": {
        "id": "hlG113MDc5Pm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Encoding categorical data"
      ],
      "metadata": {
        "id": "qS3HPSLha5Z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "labelencoder_X = LabelEncoder()\n",
        "X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0])"
      ],
      "metadata": {
        "id": "0sy5tp3pa5m_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a dummy variable"
      ],
      "metadata": {
        "id": "OWvb2dTWa51j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
        "X = onehotencoder.fit_transform(X).toarray()\n",
        "labelencoder_Y = LabelEncoder()\n",
        "Y =  labelencoder_Y.fit_transform(Y)"
      ],
      "metadata": {
        "id": "IhiWAwhydjO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Splitting the datasets into training sets and Test sets"
      ],
      "metadata": {
        "id": "mzp2b518bVi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cross_validation import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "dvhrUW-8dl7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Feature Scaling"
      ],
      "metadata": {
        "id": "LhtZM0mvbdVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "vfsICjo8cSqr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}